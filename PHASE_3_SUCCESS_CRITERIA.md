# Phase 3 Success Criteria  
_Defining when the HRM → CodeGen adaptation is “done” and ready for Phase 4 optimisation._

---

## 1 · Quantitative Success Metrics

| No. | Metric | Definition | Measurement Tool | Target (MVP) | Stretch Goal |
|-----|--------|------------|------------------|---------------|--------------|
| 1   | Pass@1 | % of MBPP **test** problems solved with k = 1 samples | `scripts/evaluate.py` | ≥ **20 %** | ≥ **30 %** |
| 2   | Pass@10 | % solved with k = 10 samples | `scripts/evaluate.py` | ≥ **35 %** | ≥ **45 %** |
| 3   | Training Stability | No gradient-error or NaN for ≥ 1 full epoch | Training logs | **0** failures | — |
| 4   | Convergence | Validation loss ↓ ≥ 30 % from start within 3 epochs | TensorBoard | Δ ≤ –30 % | Δ ≤ –40 % |
| 5   | Throughput | Tokens/sec on M1 CPU (batch = 8, seq = 256) | `scripts/benchmark.py` | ≥ **550** tok/s | ≥ 650 tok/s |
| 6   | Memory Footprint | Peak host RAM during training (CPU) | `psutil` monitor | ≤ **16 GB** | ≤ 12 GB |
| 7   | Unit-/Integration-Tests | tests passed / total | `pytest -q` | **100 %** | — |

---

## 2 · Minimum Viable Thresholds vs Stretch Goals
* **MVP Pass** = all “Target (MVP)” columns met _and_ no critical regression versus baselines (see §6).  
* **Stretch Success** = ≥ 75 % of stretch goals met.

---

## 3 · Validation Procedures & Test Cases

1. **Environment**  
   ```bash
   git clone https://github.com/zshinyg/HRMTraining.git
   git checkout droid/feature-development
   python -m venv venv && source venv/bin/activate
   pip install -r requirements.txt
   ```

2. **Dataset availability**:  
   `data/mbpp/train.bin`, `data/mbpp/test.bin` must exist and checksum match `checksums.txt`.

3. **Unit & integration tests**  
   ```bash
   pytest -q tests/
   python scripts/test_setup.py --config hrm/configs/mbpp_dev.yaml
   ```

4. **Training run** (dev config):  
   ```bash
   python scripts/train.py --config hrm/configs/mbpp_dev.yaml \
     --data-path data/mbpp/train.bin --eval-data-path data/mbpp/test.bin \
     --tokenizer-path gpt2 --max-steps 500
   ```

5. **Evaluation**  
   ```bash
   python scripts/evaluate.py --ckpt checkpoints/.../best.pt \
     --split test --k 1 10 --tokenizer-path gpt2
   ```

6. **Performance benchmark**  
   ```bash
   python scripts/benchmark.py --seq-len 256 --batch 8
   ```

---

## 4 · Success Evaluation Framework

1. **Logging**  
   * Training & eval metrics pushed to W&B project `hrm-codegen`.
   * CI pipeline posts Pass@k scores into `reports/latest.json`.

2. **Review cadence**  
   * Daily automated Slack report with metric deltas.  
   * End-of-Phase review meeting.

3. **Decision artefact**  
   * `PHASE_3_EVALUATION_REPORT.md` autogenerated by CI summarising metrics vs targets.

---

## 5 · Go / No-Go Criteria for Phase 4

| Criterion | Condition | Decision |
|-----------|-----------|----------|
| Metrics pass | All MVP targets met | **GO** |
| Any critical blocker (training crash, eval script failure, data corruption) | Not resolved within 24 h | **NO-GO** |
| Pass@1 < 15 % _or_ Pass@10 < 25 % | Despite ≥ 3 tuning attempts | **NO-GO** |
| Security issues in code execution harness | Unresolved | **NO-GO** |

---

## 6 · Performance Benchmarks & Baselines

| Model | Params | Pass@1 | Pass@10 | Notes |
|-------|--------|--------|---------|-------|
| GPT-2 small (117 M) | 117 M | 26 % | 42 % | Local baseline |
| CodeT5-small | 60 M | 22 % | 38 % | HF transformer |
| **HRM-CodeGen (this work)** | 27 M | ≥ 20 %* | ≥ 35 %* | Targets above |

\*Minimum viable; stretch ≥ 30 % / 45 %.

---

## 7 · Quality Gates & Acceptance Checklist

| Gate | Check | Tool | Status |
|------|-------|------|--------|
| Formatting | `black --check` passes | pre-commit |  |
| Linting | `ruff` score ≤ 5 warnings | CI |  |
| Tests | 100 % pass | `pytest` |  |
| Docs | Updated README & API docs | `mkdocs build` |  |
| Security | No unsafe exec patterns (bandit score) | `bandit` |  |
| License | External code retains Apache-2.0 headers | manual |  |
| Perf | Benchmarks ≥ MVP tokens/sec | `benchmark.py` |  |

_All boxes must be ✔ before Phase 3 deemed complete._

---

## Appendix A · Metric Collection Scripts
* `scripts/collect_metrics.py` – collates W&B runs into CSV  
* `scripts/benchmark.py` – measures throughput & memory  
* `scripts/check_quality.py` – aggregated quality-gate runner

---

**End of document**
