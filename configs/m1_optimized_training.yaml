# M1 Mac Optimized Training Configuration for HRM
# Targets: 27M parameter model for MBPP code generation task
# Hardware: Apple Silicon M1/M2/M3 chips with Metal Performance Shaders (MPS)

# ===== Model Configuration =====
model:
  hidden_dim: 768
  num_heads: 12
  high_level_dim: 768
  low_level_dim: 768
  high_level_layers: 2
  low_level_layers: 4
  high_level_steps: 16
  timing_ratio: 4
  dropout: 0.1
  activation: "gelu"
  initializer_range: 0.02
  vocab_size: 50257  # GPT-2 vocabulary size
  max_position_embeddings: 1024
  tie_word_embeddings: true

# ===== Data Configuration =====
data:
  train_data_path: "data/mbpp/train.bin"
  val_data_path: "data/mbpp/test.bin"  # Using test as validation
  test_data_path: "data/mbpp/test.bin"
  vocab_file: "data/mbpp/vocab.json"
  merges_file: "data/mbpp/merges.txt"
  tokenizer_type: "gpt2"
  max_seq_length: 512
  context_length: 256
  target_length: 256
  num_workers: 4
  pin_memory: false  # Not needed for MPS
  use_augmentation: false
  augmentation_factor: 1
  filter_by_length: true
  min_length: 10
  max_length: 512

# ===== Training Configuration =====
training:
  epochs: 3  # Changed from max_steps to epochs
  global_batch_size: 32
  gradient_accumulation_steps: 8  # Accumulate gradients to simulate larger batch
  optimizer: "adamw"  # Matches OptimType enum
  learning_rate: 5.0e-5
  weight_decay: 0.01
  max_grad_norm: 1.0
  scheduler: "warmup_cosine"  # Matches SchedulerType enum
  warmup_steps: 500
  warmup_ratio: 0.1
  use_mixed_precision: true
  mixed_precision_dtype: "float16"
  distributed: false
  local_rank: -1
  world_size: 1
  dropout: 0.1
  attention_dropout: 0.1
